{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:: \n",
    "\n",
    "1. build date function and convert utc to mm dd - done!!!\n",
    "\n",
    "    1.1 - instead of using date select miliseconds as test data from 11.2018 / check if at least 15% of data - done!!!\n",
    "\n",
    "\n",
    "2. remove duplicate items per single person / remove by bidid  - done my michael - done!!!\n",
    "\n",
    "\n",
    "3. remove trash columns from android_bids\n",
    "\n",
    "    3.1 remove marketplace col - done !!!\n",
    "    \n",
    "    3.2 device_maker and device_model NaN's changed to unknown and claasified as new device_maker or device_model - done!!!\n",
    "    \n",
    "    3.3 \n",
    "    \n",
    "    \n",
    "\n",
    "4. remove trash columns from play_apps\n",
    "\n",
    "    4.1 Create dataframe from files - done!!!\n",
    "    \n",
    "    4.2 Do we need to use play_apps -> Which cols do we want to use??\n",
    "    \n",
    "        [video, category, score, histogram, reviews, editors_choice, (price, free), iap, size, installs, content_rating, iap_range, interactive_elements?, bids]\n",
    "\n",
    "\n",
    "5. trun into comma seperated label for play_apps -> category\n",
    "\n",
    "\n",
    "6. Merge data from two tables android_bids_us and play_apps\n",
    "\n",
    "    6.1 USE pandas merge datafreames function to merge both files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install once\n",
    "#!pip install imbalanced-learn --user\n",
    "#!pip install delayed --user\n",
    "\n",
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import pytz\n",
    "import csv\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MultiLabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read orig android data\n",
    "origdata = pd.read_csv('data/android_bids_us.csv')\n",
    "origdata.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze android dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTypes = origdata.dtypes\n",
    "print(f'Total columns: {dataTypes.size} ||| Total rows: {origdata.count()}')\n",
    "print(f'Data type of each column of Dataframe : ')\n",
    "print(dataTypes)\n",
    "# We may have NAN's in user_isp, device_maker, device_model\n",
    "\n",
    "print('-----------------------------------')\n",
    "print('Amount of NaN under utc_time', origdata['utc_time'].isnull().sum())\n",
    "# all items in marketplace col equal 'chartboost'. we can remove col\n",
    "\n",
    "#Print start time \n",
    "start_time = origdata['utc_time'].min()\n",
    "end_time = origdata['utc_time'].max()\n",
    "print('Start Time: ',  datetime.fromtimestamp(start_time/1000))\n",
    "print('End Time: ',  datetime.fromtimestamp(end_time/1000))\n",
    "\n",
    "print('-----------------------------------')\n",
    "\n",
    "print('-----------------------------------')\n",
    "print('Variance data of android dataset')\n",
    "print(origdata.var(ddof=0))\n",
    "print('-----------------------------------')\n",
    "\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(origdata.marketplace.unique())\n",
    "# all items in marketplace col equal 'chartboost'. we can remove col\n",
    "print('-----------------------------------')\n",
    "\n",
    "print('-----------------------------------')\n",
    "print('app_id::')\n",
    "print('Amount of NaN under app_id', origdata['app_id'].isnull().sum())\n",
    "print(origdata.app_id.value_counts())\n",
    "print('-----------------------------------')\n",
    "\n",
    "print('-----------------------------------')\n",
    "print('user_isp::')\n",
    "print('Amount of NaN under user_isp', origdata['user_isp'].isnull().sum())\n",
    "print(origdata.user_isp.value_counts())\n",
    "print('-----------------------------------')\n",
    "\n",
    "print('-----------------------------------')\n",
    "print('user_state::')\n",
    "print(origdata.user_state.value_counts())\n",
    "#Consider use onehotencoding insted of laelencoding\n",
    "print('-----------------------------------')\n",
    "\n",
    "print('-----------------------------------')\n",
    "print('device_maker::')\n",
    "print(origdata.device_maker.value_counts())\n",
    "print('-----------------------------------')\n",
    "\n",
    "print('-----------------------------------')\n",
    "print('device_model::')\n",
    "print(origdata.device_model.value_counts())\n",
    "print('-----------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Time Zones dictionary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_zones_csv = {}\n",
    "\n",
    "with open('time_zones.csv', mode='r') as inp:\n",
    "    reader = csv.reader(inp)\n",
    "    time_zones_csv = {rows[0]:rows[2] for rows in reader}\n",
    "\n",
    "print(time_zones_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare android dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rearrange dataframe\n",
    "#clone orig dataframe\n",
    "android_df = origdata.copy()\n",
    "\n",
    "#-----------------------------\n",
    "\n",
    "#Add date from utc and remove utc\n",
    "#android_df['date'] = android_df['utc_time'].apply(lambda x: datetime.fromtimestamp(x/1000))\n",
    "#android_df = android_df.drop(['utc_time'], axis=1)\n",
    "\n",
    "# this col will be removed in the last part after splitting the data\n",
    "\n",
    "#-----------------------------\n",
    "\n",
    "\n",
    "#-----------------------------\n",
    "# Fix 'utc_time' based on 'user_state'\n",
    "\n",
    "def update_time_utc(time_in_mili, state):\n",
    "    current_timezone = pytz.timezone(time_zones_csv[state])\n",
    "    localized_timestamp = current_timezone.localize(datetime.fromtimestamp(time_in_mili / 1000))\n",
    "    \n",
    "    return localized_timestamp.timestamp() * 1000\n",
    "android_df['utc_time'] = android_df.apply(lambda x:update_time_utc( x.utc_time, x.user_state), axis=1)\n",
    "\n",
    "#-----------------------------\n",
    "\n",
    "\n",
    "#-----------------------------\n",
    "# Manipulate time column to extract hour from time. \n",
    "def utc_to_local(utc_dt):\n",
    "    return datetime.fromtimestamp(utc_dt/1000).hour\n",
    "\n",
    "# Add 'hour' column.\n",
    "android_df['hour'] = android_df['utc_time'].apply(lambda x: utc_to_local(x))\n",
    "\n",
    "#-----------------------------\n",
    "\n",
    "\n",
    "#Remove marketplace col -> all values are the same\n",
    "android_df = android_df.drop(['marketplace'], axis=1)\n",
    "\n",
    "#-----------------------------\n",
    "\n",
    "#Remove duplicated bidid. Keep only single instance keep the one with click == 1\n",
    "android_df = android_df[~((android_df['bidid'].duplicated(keep=False))&(android_df['click']==0))]\n",
    "#and then remove bids column\n",
    "android_df = android_df.drop(['bidid'], axis=1)\n",
    "\n",
    "#-----------------------------\n",
    "\n",
    "#-----------------------------\n",
    "#Change device_maker NaN's to unknown manufacturerer\n",
    "#android_df['device_maker'] = android_df['device_maker'].apply(lambda x: 'unknown' if x!=x else x)\n",
    "android_df.drop('device_maker', axis = 1,inplace = True)\n",
    "\n",
    "#Change device_model NaN's to unknown valid model\n",
    "#android_df['device_model'] = android_df['device_model'].apply(lambda x: 'unknown' if x!=x else x)\n",
    "android_df.drop('device_model', axis = 1,inplace = True)\n",
    "#-----------------------------\n",
    "\n",
    "#-----------------------------\n",
    "# Remove device_osv column\n",
    "android_df.drop('device_osv', axis = 1,inplace = True)\n",
    "#-----------------------------\n",
    "\n",
    "#-----------------------------\n",
    "# Take size cols and transform into resolution\n",
    "android_df['screen_resolution'] = android_df['device_height']*android_df['device_width']\n",
    "android_df.drop(['device_width', 'device_height'], axis=1, inplace=True)\n",
    "\n",
    "# And then do scaling to new col\n",
    "scaler = StandardScaler()\n",
    "android_df['screen_resolution'] = scaler.fit_transform(android_df[['screen_resolution']])\n",
    "#-----------------------------\n",
    "\n",
    "#-----------------------------\n",
    "# LabelEncoder for user_state\n",
    "#Consider use onehotencoding insted of laele ncoding\n",
    "le1 = LabelEncoder()\n",
    "android_df['user_state'] = le1.fit_transform(android_df['user_state'])\n",
    "#-----------------------------\n",
    "\n",
    "\n",
    "#-----------------------------\n",
    "# user_isp\n",
    "# LabelEncoder for user_isp\n",
    "\n",
    "#Consider use onehotencoding insted of laele ncoding\n",
    "#android_df['user_isp'] = android_df['user_isp'].fillna(method=\"ffill\")\n",
    "#le2 = LabelEncoder()\n",
    "#android_df['user_isp'] = le2.fit_transform(android_df['user_isp'])\n",
    "\n",
    "android_df.drop('user_isp', axis = 1,inplace = True)\n",
    "#-----------------------------\n",
    "\n",
    "\n",
    "android_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Click percent in data: {android_df.click.sum()/android_df.shape[0]*100}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract play_apps data and first clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get play_apps data\n",
    "\n",
    "\n",
    "#Read orig play_apps data\n",
    "#app_details_orig = ZipFile(\"data/play_apps.zip\")\n",
    "#app_file = 'play_apps/a008.com.fc2.blog.androidkaihatu.datecamera2'\n",
    "#app = pickle.loads(app_details_orig.read(app_file))\n",
    "#app\n",
    "\n",
    "\n",
    "def zipToDataFrame(zipped, to_remove):\n",
    "    rows = []\n",
    "    \n",
    "    for f in zipped.filelist:\n",
    "        app_file = f.filename\n",
    "        try:\n",
    "            app = pickle.loads(zipped.read(app_file))\n",
    "        except EOFError as e:\n",
    "            continue\n",
    "        except pickle.UnpicklingError as e:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            return\n",
    "            \n",
    "        # Remove columns\n",
    "        for col in to_remove:\n",
    "            del app[col]\n",
    "        rows.append(app)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "app_details = ZipFile(\"data/play_apps.zip\")\n",
    "cols_to_remove = ['title','screenshots', 'icon', 'size',\n",
    "                  'histogram', 'description', 'description_html', \n",
    "                  'recent_changes', 'developer', 'developer_id', 'developer_email', 'developer_url',\n",
    "                  'developer_address', 'url', 'current_version', 'updated', 'required_android_version',\n",
    "                  'iap_range']\n",
    "\n",
    "\n",
    "orig_play_apps = zipToDataFrame(app_details, cols_to_remove)\n",
    "orig_play_apps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze play_apps dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore play_apps data characteristics\n",
    "\n",
    "apps_dataTypes = orig_play_apps.dtypes\n",
    "\n",
    "print(f'Total columns: {apps_dataTypes.size} ||| Total rows: {orig_play_apps.count()}')\n",
    "print(f'Data type of each column of Dataframe : ')\n",
    "print(apps_dataTypes)\n",
    "# We may have NAN's in iap_range, interactive_elements, price\n",
    "\n",
    "print('-----------------------------------')\n",
    "print('Amount of NaN under price', orig_play_apps['price'].isnull().sum())\n",
    "print('Amount of NaN under free', orig_play_apps['free'].isnull().sum())\n",
    "print('price values: ', orig_play_apps.price.unique())\n",
    "print('free values: ',orig_play_apps.free.unique())\n",
    "\n",
    "print('How many free from all dataset: ', (orig_play_apps['free'] == True).sum())\n",
    "# items in price and free should be in a single col\n",
    "# we can take the free col since the non free item is very little : about 1000 out of 29000\n",
    "# or we can take col price since 0 represnts free\n",
    "print('-----------------------------------')\n",
    "\n",
    "print('-----------------------------------')\n",
    "print('Variance data of play aps dataset')\n",
    "print(orig_play_apps.var(ddof=0))\n",
    "print('-----------------------------------')\n",
    "\n",
    "print('-----------------------------------')\n",
    "# items in price and free should be in a single col\n",
    "print('Amount of NaN under score', orig_play_apps['score'].isnull().sum())\n",
    "print('score: ', orig_play_apps.score.unique())\n",
    "print('-----------------------------------')\n",
    "\n",
    "\n",
    "print('-----------------------------------')\n",
    "# Test reviews col - need to be scaled\n",
    "print('Amount of NaN under reviews', orig_play_apps['reviews'].isnull().sum())\n",
    "print('reviews: ', orig_play_apps.reviews.unique())\n",
    "print('-----------------------------------')\n",
    "\n",
    "print('-----------------------------------')\n",
    "# Removed for testing\n",
    "# Test bids col - need to be scaled\n",
    "print('Amount of NaN under bids', orig_play_apps['bids'].isnull().sum())\n",
    "print('bids: ', orig_play_apps.bids.unique())\n",
    "print('-----------------------------------')\n",
    "\n",
    "print('-----------------------------------')\n",
    "# Removed for testing\n",
    "print('content_rating column::: ')\n",
    "# Test content_rating col\n",
    "rating = orig_play_apps['content_rating'].agg(lambda x: ':'.join(map(str, x)))\n",
    "print(rating.unique())\n",
    "# We might drop this column since its require a lot of handling and im not sure its gonna impact our model\n",
    "print('-----------------------------------')\n",
    "\n",
    "\n",
    "print('-----------------------------------')\n",
    "print('category column::: ')\n",
    "# Test category col\n",
    "category = orig_play_apps['category'].agg(lambda x: ':'.join(map(str, x)))\n",
    "print(category.unique())\n",
    "# We might drop this column since its require a lot of handling and im not sure its gonna impact our model\n",
    "print('-----------------------------------')\n",
    "\n",
    "print('-----------------------------------')\n",
    "print('interactive_elements column::: ')\n",
    "# Test category col\n",
    "print('Amount of NaN under interactive_elements', orig_play_apps['interactive_elements'].isnull().sum())\n",
    "interactive = orig_play_apps['interactive_elements'].agg(lambda x: ':'.join(map(str, x)))\n",
    "interactive\n",
    "#print(interactive.unique())\n",
    "# We might drop this column since its require a lot of handling and im not sure its gonna impact our model\n",
    "print('-----------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare play_apps dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix and manipulate play_apps data before merge with android_bids_us\n",
    "#clone orig dataframe\n",
    "play_apps_df = orig_play_apps.copy()\n",
    "\n",
    "# Convert String to int.\n",
    "def convert_installs(str_val=''):\n",
    "    str_val = str_val.translate({ord(i):None for i in ',+'})\n",
    "    return int(str_val)\n",
    "\n",
    "# Convert String to float.\n",
    "def convert_price(str_val=''):\n",
    "    str_val = str_val.translate({ord(i):None for i in '$'})\n",
    "    return float(str_val)\n",
    "\n",
    "\n",
    "#---------------------------------------------------\n",
    "# Handle 'Installs' column\n",
    "# Translate to number and then scale it\n",
    "play_apps_df['installs'].fillna(method='pad', inplace=True)\n",
    "play_apps_df['installs'] = play_apps_df['installs'].apply(lambda s: convert_installs(s))\n",
    "\n",
    "# And then do scaling to new col\n",
    "scaler = StandardScaler()\n",
    "play_apps_df['installs'] = scaler.fit_transform(play_apps_df[['installs']])\n",
    "#---------------------------------------------------\n",
    "\n",
    "\n",
    "#---------------------------------------------------\n",
    "# Handle 'video' column\n",
    "play_apps_df['video'] = play_apps_df['video'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "#---------------------------------------------------\n",
    "\n",
    "#---------------------------------------------------\n",
    "# Handle 'score ' column\n",
    "# Translate to number\n",
    "play_apps_df['score'] = pd.to_numeric(play_apps_df['score'],errors = 'coerce')\n",
    "play_apps_df['score'].fillna(method='pad', inplace=True)\n",
    "play_apps_df['score'] = scaler.fit_transform(play_apps_df[['score']])\n",
    "#---------------------------------------------------\n",
    "\n",
    "#---------------------------------------------------\n",
    "# Handle 'reviews' column - we cant know weither the review is good or bad, \n",
    "# but the amount of reviews can point on popularity off the app\n",
    "# There are no NaN's\n",
    "# Translate to number\n",
    "scaler = StandardScaler()\n",
    "play_apps_df['reviews'] = scaler.fit_transform(play_apps_df[['reviews']])\n",
    "#---------------------------------------------------\n",
    "\n",
    "\n",
    "#---------------------------------------------------\n",
    "#price and free can be merged into single col named price since 0 represntes free app\n",
    "# and scale it\n",
    "play_apps_df['price'] = play_apps_df['price'].fillna(method='pad', inplace=False)\n",
    "play_apps_df['price'] = play_apps_df['price'].apply(lambda s: convert_price(s))\n",
    "play_apps_df['price'] = scaler.fit_transform(play_apps_df[['price']])\n",
    "play_apps_df.drop(['free'], axis=1, inplace=True)\n",
    "#---------------------------------------------------\n",
    "\n",
    "\n",
    "#---------------------------------------------------\n",
    "# 'category' column transform to multil-lable\n",
    "# Removed for testing\n",
    "'''mlb = MultiLabelBinarizer()\n",
    "mlb.fit(play_apps_df['category'])\n",
    "new_col_names = mlb.classes_\n",
    "\n",
    "# Create new DataFrame with transformed/one-hot encoded categories\n",
    "categories = pd.DataFrame(mlb.fit_transform(play_apps_df['category']), columns=new_col_names)\n",
    "\n",
    "# Concat with original `category` column\n",
    "play_apps_df = pd.concat([play_apps_df, categories], axis=1 )\n",
    "play_apps_df.drop('category', axis=1,inplace=True)'''\n",
    "#---------------------------------------------------\n",
    "\n",
    "\n",
    "#Temporary removed columns\n",
    "play_apps_df.drop(['bids'], axis=1, inplace=True)\n",
    "play_apps_df.drop(['content_rating'], axis=1, inplace=True)\n",
    "play_apps_df.drop(['interactive_elements'], axis=1, inplace=True)\n",
    "play_apps_df.drop('category', axis=1,inplace=True)\n",
    "\n",
    "\n",
    "play_apps_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------------------------------------')\n",
    "#merge two data sets into merged dataframe\n",
    "merged_df = pd.merge(android_df, play_apps_df, on='app_id')\n",
    "merged_df = merged_df.drop(['app_id'], axis=1)\n",
    "\n",
    "print('Rows before merge: ',android_df.shape[0])\n",
    "print('Rows after merge: ', merged_df.shape[0])\n",
    "\n",
    "#Concolusion after merge\n",
    "# from android = 2936921 rows × 10 columns turn into 2656577 rows × 24 columns\n",
    "# we missed ~300k rows. need to check y.....\n",
    "print('---------------------------------------------')\n",
    "\n",
    "\n",
    "merged_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data and save to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------------------------------------')\n",
    "print('Amount of NaN under utc_time: ', merged_df['utc_time'].isnull().sum())\n",
    "print('Amount of NaN in all data : ', merged_df.isnull().sum().sum())\n",
    "print('---------------------------------------------')\n",
    "\n",
    "\n",
    "print('---------------------------------------------')\n",
    "#Print start time after merge\n",
    "merged_start_time = merged_df['utc_time'].min()\n",
    "merged_end_time = merged_df['utc_time'].max()\n",
    "print('Start Time: ',  datetime.fromtimestamp(merged_start_time/1000))\n",
    "print('End Time: ',  datetime.fromtimestamp(merged_end_time/1000))\n",
    "print('Split time: ',  datetime.fromtimestamp(1541023200000/1000))\n",
    "print('---------------------------------------------')\n",
    "\n",
    "\n",
    "print('---------------------------------------------')\n",
    "tarin_df, test_df = merged_df[(mask:=merged_df['utc_time'] < 1541023200000)], merged_df[~mask]\n",
    "\n",
    "tarin_df = tarin_df.drop(['utc_time'], axis=1)\n",
    "test_df = test_df.drop(['utc_time'], axis=1)\n",
    "\n",
    "tarin_size = tarin_df.shape[0]\n",
    "test_size = test_df.shape[0]\n",
    "\n",
    "print('Amount of tarin_df data : ', tarin_size)\n",
    "print('Amount of test_df data : ', test_size)\n",
    "\n",
    "print(f'Test data is: {(test_size / (tarin_size + test_size))*100} percent')\n",
    "\n",
    "      \n",
    "print('---------------------------------------------')\n",
    "      \n",
    "#Save to CSV files\n",
    "#Save to splited csv files\n",
    "filepath1 = Path('data/train.csv')  \n",
    "filepath2 = Path('data/test.csv')  \n",
    "tarin_df.to_csv(filepath1, index=False)\n",
    "test_df.to_csv(filepath2, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only if not installed\n",
    "#!pip install imbalanced-learn\n",
    "#!pip install delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to eliminate not balanced clicks\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "oversample_smote = SMOTE()\n",
    "X_smote, y_smote = oversample_smote.fit_resample(merged_df.drop('click', axis=1), merged_df['click'])\n",
    "\n",
    "y_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new DataFrame from SMOTE\n",
    "\n",
    "merged_smote_df = X_smote.copy()\n",
    "merged_smote_df['click'] =  y_smote\n",
    "train_smote_df, test_smote_df = merged_df[(mask:=merged_smote_df['utc_time'] < 1541023200000)], merged_smote_df[~mask]\n",
    "\n",
    "train_smote_df = train_smote_df.drop(['utc_time'], axis=1)\n",
    "test_smote_df = test_smote_df.drop(['utc_time'], axis=1)\n",
    "\n",
    "print('Amount of train SMOTE_df data : ', train_smote_df.shape[0])\n",
    "print('Amount of test SMOTE data : ', test_smote_df.shape[0])\n",
    "\n",
    "print('---------------------------------------------')\n",
    "      \n",
    "#Save to CSV files\n",
    "#Save to splited csv files\n",
    "filepath1 = Path('data/train_SMOTE.csv')  \n",
    "filepath2 = Path('data/test_SMOTE.csv')  \n",
    "train_smote_df.to_csv(filepath1, index=False)\n",
    "test_smote_df.to_csv(filepath2, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
